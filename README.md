# BERTSpamDetection
The goal of this project is to finetune BertForSequenceClassification to classify english spam emails. Companies deal with an immense amount of emails everyday, in which spam emails can creep into this flood of emails. They are very difficult and inefficient to identify manually. A large number of spam emails nowadays are cyber attacks, which poses large financial, operational and reputational risks for companies. By using a finetuned BERT model to automatically detect spam emails, companies increase their security by minimizing the risk of cyber security attacks while saving increasing the productivity of employees.

#### Dataset 
A public dataset from Kaggle with 5.171 english emails, out of which 41% are spam emails were used to finetune the BERT model after applying appropriate data transformations 

#### Evaluation
The effect of the finetuning process was evaluated by comparing the spam detection quality of base BERT and finetuned BERT. Both were used to classify spam emails from 3 different sources:
- sample of 50 spam emails from training dataset -> Finetuned BERT is expected to achieve 100% accuracy
- 40 spam emails generated by ChatGPT -> Since the content is generated by another LLM, there might be similarities in language pattern which leads to a high accuracy
- 14 private spam emails -> Most realistic test and lowest accuracy expected but small sample

#### Results
With finetuned BERT all evaluation datasets are classified as spam correctly. Base BERT only classified the ChatGPT evaluation dataset 100% correct, which supports the claim that content generated by LLMs have similar language patterns. The biggest performance difference came with the private spam emails, where base BERT classified 11 out of 14 samples correct. In summary the finetuning process of BERT led to a performance increase for spam email detection.

![image](https://github.com/hoangnghiem17/BERTSpamDetection/assets/118189008/efd183e0-91c7-4c5d-990d-4b0ce0e7873a)

