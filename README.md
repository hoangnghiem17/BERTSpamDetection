# BERTSpamDetection

## Overview

The goal of this project is to finetune BertForSequenceClassification to classify english spam emails (https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification).

Companies deal with an immense amount of emails everyday, in which spam emails can creep into. They are very difficult and inefficient to identify manually. A large number of spam emails nowadays are cyber attacks, which poses large financial, operational and reputational risks for companies. By using a finetuned BERT model to automatically detect spam emails, companies minimize the risk of cyber security attacks while increasing the productivity of employees.

## Key Methods

- **Transfer Learning**: Leverages pre-trained BERT for spam detection
- **Multi-Dataset Evaluation**: Tests on real, synthetic, and private data
- **Comprehensive Metrics**: Multiple evaluation criteria for thorough assessment
- **Visualization**: Training curves and performance comparisons

## Technical Architecture

### Model Overview

This project implements a **BERT-based spam detection system** using the `BertForSequenceClassification` model from the Hugging Face Transformers library. The architecture consists of:

- **Base Model**: `bert-base-uncased` (12-layer, 768-hidden, 12-heads, 110M parameters)
- **Classification Head**: Binary classification (Spam/Ham) with 2 output labels
- **Tokenization**: BERT tokenizer with max sequence length of 32 tokens
- **Training**: Fine-tuning approach with AdamW optimizer

### Implementation Details

#### Data Preprocessing
- **Text Cleaning**: Removes special characters, URLs, and normalizes whitespace
- **Tokenization**: BERT tokenizer with lowercase processing
- **Sequence Padding**: Fixed length sequences (32 tokens) with attention masks
- **Data Split**: 80% training, 20% validation with stratified sampling

#### Model Configuration
```python
# Model Parameters
max_length = 32
batch_size = 16
learning_rate = 5e-5
epochs = 4
num_labels = 2  # Binary classification
```

#### Training Process
- **Optimizer**: AdamW with learning rate 5e-5
- **Loss Function**: Cross-entropy loss
- **Evaluation Metrics**: Accuracy, Precision, Recall, Specificity
- **Device**: GPU acceleration when available, CPU fallback

## Dataset and Evaluation

### Dataset Structure

The project uses three datasets for evaluation:

1. **Training Dataset** (`spam_ham_dataset.csv`)
   - 5,171 English emails
   - 41% spam, 59% ham
   - Source: Kaggle public dataset (https://www.kaggle.com/code/ayhampar/spam-ham-dataset/input)

2. **ChatGPT Generated Spam** (`spam_chatgpt_v2.xlsx`)
   - 40 synthetic spam emails
   - Generated using ChatGPT for testing LLM pattern recognition

3. **Private Spam Collection** (`own_spam.xlsx`)
   - 14 real-world spam emails
   - Most challenging test case

### Evaluation Methodology

The effect of the finetuning process was evaluated by comparing the spam detection quality of base BERT and finetuned BERT. Both were used to classify spam emails from 3 different sources:

- **Training Sample**: 50 spam emails from training dataset → Finetuned BERT is expected to achieve 100% accuracy
- **ChatGPT Generated**: 40 spam emails generated by ChatGPT → Since the content is generated by another LLM, there might be similarities in language pattern which leads to a high accuracy
- **Private Collection**: 14 private spam emails → Most realistic test and lowest accuracy expected but small sample

### Performance Metrics

The model evaluation includes comprehensive metrics:

- **Accuracy**: Overall correct predictions
- **Precision**: True positives / (True positives + False positives)
- **Recall**: True positives / (True positives + False negatives)
- **Specificity**: True negatives / (True negatives + False positives)

## Results and Performance

### Key Findings

With finetuned BERT all evaluation datasets are classified as spam correctly. Base BERT only classified the ChatGPT evaluation dataset 100% correctly, which supports the claim that content generated by LLMs have similar language patterns. The biggest performance difference came with the private spam emails, where base BERT classified 11 out of 14 samples correct. In summary the finetuning process of BERT led to a performance increase for spam email detection.

![image](https://github.com/hoangnghiem17/BERTSpamDetection/assets/118189008/efd183e0-91c7-4c5d-990d-4b0ce0e7873a)

### Performance Results

| Dataset | Base BERT | Fine-tuned BERT | Improvement |
|---------|-----------|-----------------|-------------|
| Training Sample | ~85% | 100% | +15% |
| ChatGPT Generated | 100% | 100% | 0% |
| Private Spam | 78.6% | 100% | +21.4% |

The fine-tuning process demonstrates significant performance improvements, especially on real-world spam detection scenarios.

## Setup and Installation

### Prerequisites
- Python 3.7+
- PyTorch 1.8+
- Transformers library
- CUDA-compatible GPU (optional, for faster training)

### Required Dependencies
```bash
pip install torch transformers pandas numpy scikit-learn matplotlib tabulate tqdm openpyxl
```

### File Structure
```
BERTSpamDetection/
├── BERT_finetuning_spamclassification.ipynb  # Main implementation
├── input/
│   ├── spam_ham_dataset.csv                 # Training dataset
│   ├── spam_chatgpt_v2.xlsx                 # ChatGPT test data
│   └── own_spam.xlsx                        # Private test data
└── README.md
```

## Usage Instructions

### 1. Environment Setup
```bash
# Clone the repository
git clone <repository-url>
cd BERTSpamDetection

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Preparation
- Ensure all dataset files are in the `input/` directory
- The notebook will automatically load and preprocess the data

### 3. Model Training
- Open `BERT_finetuning_spamclassification.ipynb` in Jupyter
- Run cells sequentially to:
  - Load and preprocess data
  - Initialize BERT model
  - Train the model (4 epochs)
  - Evaluate performance

### 4. Evaluation
- The notebook includes comprehensive evaluation on all three datasets
- Performance comparison between base and fine-tuned BERT
- Visualization of training progress and results

