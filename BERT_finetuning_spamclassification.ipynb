{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59ed48",
   "metadata": {},
   "source": [
    "# 1) Auswahl Datensatz für das Finetuning\n",
    "- Quelle: 4 Datensätze aus Kaggle\n",
    "- Exploration der Datensätze\n",
    "- Auswahl des besten Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.read_csv('emails.csv')\n",
    "spam = pd.read_excel('spam.xlsx')\n",
    "spam_ham_dataset = pd.read_csv('spam_ham_dataset.csv')\n",
    "spam_or_not_spam = pd.read_csv('spam_or_not_spam.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb2652",
   "metadata": {},
   "source": [
    "#### 1) emails - nicht geeignet, da Aufbau wie Dokumenterm-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails.info()\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_spam = emails[(emails['Prediction']) == 1]\n",
    "len(emails_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ff29f",
   "metadata": {},
   "source": [
    "#### 2) spam - nicht geeignet, da weitere Datentransformationen nötig und keine gute Datenqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = spam[['v1,v2,']]\n",
    "spam.info()\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb63e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_search = 'ham'\n",
    "rows_containing_word = spam['v1,v2,'].str.contains(word_to_search, case=False).sum()\n",
    "\n",
    "print(f\"Number of rows containing '{word_to_search}': {rows_containing_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b685c7",
   "metadata": {},
   "source": [
    "#### 3) spam_ham_dataset - Winner durch geeignete Struktur, Subject-Kennung und beste Datenqualität unter allen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_ham_dataset = spam_ham_dataset.drop('Unnamed: 0', axis=1)\n",
    "spam_ham_dataset.info()\n",
    "spam_ham_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam_ham_dataset['text'][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d185731",
   "metadata": {},
   "source": [
    "#### 4) spam_or_not_spam - Struktur ist geeignet aber Datenqualität nicht gut, oft Nachrichten anstatt E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_or_not_spam.info()\n",
    "spam_or_not_spam.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6147c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spam = spam_or_not_spam[(spam_or_not_spam['label']) == 1]\n",
    "len(test_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam_or_not_spam['email'][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89762046",
   "metadata": {},
   "source": [
    "# 2) Ausgewählten Datensatz \"spam_ham_dataset\" preprocessen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05a678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spam_ham_dataset.drop('label', axis=1)\n",
    "df.rename(columns={'label_num': 'label'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee6d68",
   "metadata": {},
   "source": [
    "#### zu entfernen\n",
    "- \\r\n",
    "- \\n\n",
    "- Alle Sonderzeichen nachdem 8. Zeichen (davor Subject:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47be09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    df['text'] = df['text'].str.replace(r'[\\r\\n\\t]+', '', regex=True) # \\r \\n \\t \n",
    "    df['text'] = df['text'].str.slice(0, 8) + df['text'].str.slice(8).str.replace(r'[^a-zA-Z0-9\\s]+', '', regex=True) # Sonder\n",
    "    df['text'] = df['text'].str.replace(r'\\bhttp\\S*\\b', '', regex=True) # Links\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce380113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaning(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1fee4",
   "metadata": {},
   "source": [
    "# 3) Preprocessing der Evaluationsstichproben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa1009",
   "metadata": {},
   "source": [
    "#### Stichprobe von 50 Spam E-Mails aus den Datensatz \"spam_ham_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df[df['label'] == 1]\n",
    "print(len(df))\n",
    "print(len(df_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffb456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_kaggle = df_spam['text'].sample(50, random_state=123)\n",
    "sample_kaggle.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba13b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, text in sample_kaggle.items():\n",
    "    print(f\"Index: {index} \\nText: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea631d9c",
   "metadata": {},
   "source": [
    "#### 40 von ChatGPT generierte Spam E-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec28a3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_chatgpt = pd.read_excel('spam_chatgpt_v2.xlsx')\n",
    "sample_chatgpt = sample_chatgpt['email']\n",
    "sample_chatgpt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c574fbd",
   "metadata": {},
   "source": [
    "#### 14 eigene Spam E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063642e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_spam = pd.read_excel('own_spam.xlsx')\n",
    "own_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33bfd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "own_spam = cleaning(own_spam)\n",
    "own_spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd576595",
   "metadata": {},
   "source": [
    "# 4) Preprocessing mit BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423a2a6",
   "metadata": {},
   "source": [
    "#### BERT Tokenizer Bibliothek führt folgende Preprocessing-Schritte durch:\n",
    "- Tokenisierung: Inputtext in einzelne Tokens runterbrechen\n",
    "- Wordpiece Encoding: nutzt Wordpiece-Algorithmus zur Aufteilung von Tokens in Subwörter\n",
    "- Spezielle Token: CLS (Anfang), SEP (Trennung zwischen Sätzen/Segmenten)\n",
    "- Padding und Truncation: behandelt Eingabesequenz, sodass alle Sequenzen gleich lang sind\n",
    "- Token IDs und Attention Mask: Zuweisung von einzigartigen Token IDs und zu beachtende Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51aa20",
   "metadata": {},
   "source": [
    "#### Eine zufällige, vom Tokenizer aufbereite E-Mail sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24515bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rand_sentence():\n",
    "  '''Displays the tokens and respective IDs of a random text sample'''\n",
    "  index = random.randint(0, len(text)-1)\n",
    "  table = np.array([tokenizer.tokenize(text[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "  print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc64f2",
   "metadata": {},
   "source": [
    "#### Mit der Funktion encode_plus von Tokenizer Preprocessing durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374082c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 32, # longer sentences truncated, shorter populated with PAD tokens\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "\n",
    "for sample in text:\n",
    "  encoding_dict = preprocessing(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa33958",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02506623",
   "metadata": {},
   "source": [
    "#### Kodierte Eingabesequenz annschauen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afa693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3ca3c",
   "metadata": {},
   "source": [
    "# 5) Finetuning von BertForSequenceClassification auf Spamerkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e97fd7",
   "metadata": {},
   "source": [
    "#### Durchgeführte Schritte:\n",
    "- 80/20 Split\n",
    "- Daten als torch.utils.data.DataLoder Objekt wrappen\n",
    "- batch_size = Anzahl Stichproben je Batch im Training\n",
    "- train_set and val_set = erstellt mit TensorDataset und kombiniert input token IDs, attention masks und labels\n",
    "- train_dataloader and validation_dataloader = erstellt mit DataLoader von torch.utils.data > Iteration über Batches während Training und Validierung\n",
    "- RandomSampler und SequentialSampler = zufällig Daten vom Datensatz samplen für Training und Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "\n",
    "batch_size = 16 # Empfohlene batch size nach BERT-Studie: 16, 32\n",
    "\n",
    "# Stratifiziertes Sampling\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Training und Validierung Datensatz\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Als DataLoader-Objekt\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88270f22",
   "metadata": {},
   "source": [
    "#### BertForSequenceClassification herunterladen mit empfohlenen Parametern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertForSequenceClassification modell laden ohne Finetuning - Base BERT\n",
    "model_base = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Vorgeschlagene Learning rates (Adam) gemäß originalem Paper: 5e-5, 3e-5, 2e-5\n",
    "optimizer = torch.optim.AdamW(model_base.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Modellparameter ausgeben\n",
    "params = list(model_base.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Running on GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertForSequenceClassification modell laden mit Finetuning - Finetuned BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06409a68",
   "metadata": {},
   "source": [
    "#### Finetuning sowie Ausgabe von Evaluationsmetriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 4\n",
    "\n",
    "train_losses = []       # List to store training losses\n",
    "val_losses = []         # List to store validation losses\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    # average training loss > training loss / number of training steps in epoch\n",
    "    train_losses.append(tr_loss / nb_tr_steps) \n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tracking variables\n",
    "    val_loss = 0\n",
    "    nb_val_examples, nb_val_steps = 0, 0\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask,\n",
    "                              labels = b_labels)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Update tracking variables\n",
    "        val_loss += eval_output.loss.item()\n",
    "        nb_val_examples += b_input_ids.size(0)\n",
    "        nb_val_steps += 1\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "            \n",
    "    val_losses.append(val_loss / nb_val_steps) # average evalluation loss > evaluation loss / number of evaluation steps in epoch\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.8f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation loss: {:.4f}'.format(val_loss / nb_val_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e939b",
   "metadata": {},
   "source": [
    "#### Finetuningprozess evaluieren anhand training und validation loss je Epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b52048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "ax1.plot((range(1, epochs + 1)), train_losses, marker='o', linestyle='-')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_xticks(range(1,5))\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Average Training Loss per Epoch')\n",
    "\n",
    "ax2.plot((range(1, epochs + 1)), val_losses, marker='o', linestyle='-')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_xticks(range(1,5))\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Average Validation Loss per Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad5f87",
   "metadata": {},
   "source": [
    "# 6) Spamvorhersage Evaluationsstichproben mit base und finetuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c4a2f",
   "metadata": {},
   "source": [
    "## Vorhersagen mit base BERT machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(spam, total):\n",
    "    return spam / total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa450643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email_base(new_sentence):\n",
    "    '''Nimmt einen Text-String und macht Vorhersagen mit base BERT modell ob spam/ham'''\n",
    "    \n",
    "    # Token IDs und Attention Mask benötigt für Inferenz auf die neue Email\n",
    "    test_ids = [] \n",
    "    test_attention_mask = []\n",
    "    \n",
    "    encoding = preprocessing(new_sentence, tokenizer) # Tokenizer anwenden\n",
    "    \n",
    "    # Token IDs und Attention Mask extrahieren\n",
    "    test_ids.append(encoding['input_ids'])\n",
    "    test_attention_mask.append(encoding['attention_mask'])\n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "    \n",
    "    # Forward pass und Berechnung von logit Vorhersagen\n",
    "    with torch.no_grad():\n",
    "        output = model_base(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "        \n",
    "    prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n",
    "    return new_sentence, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9eefd",
   "metadata": {},
   "source": [
    "#### Stichprobe von 50 Spam E-Mails aus den Datensatz \"spam_ham_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_kaggle_base = []\n",
    "prediction_kaggle_base = []\n",
    "for index, text in sample_kaggle.items():\n",
    "    email, prediction = classify_email_base(text)\n",
    "    email_kaggle_base.append(email)\n",
    "    prediction_kaggle_base.append(prediction)\n",
    "    \n",
    "results_kaggle_base = pd.DataFrame(list(zip(email_kaggle_base, prediction_kaggle_base)), columns = ['email', 'spam_prediction'])\n",
    "results_kaggle_base.head()\n",
    "\n",
    "spam = results_kaggle_base['spam_prediction'] == 'Spam'\n",
    "spam_df = results_kaggle_base[spam]\n",
    "accuracy_kaggle_base = calculate_accuracy(len(spam_df), len(results_kaggle_base))*100\n",
    "print(f\"Accuracy base BERT auf Kaggle Datensatz: {accuracy_kaggle_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39e4b8",
   "metadata": {},
   "source": [
    "#### 40 von ChatGPT generierte Spam E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_chatgpt_base = []\n",
    "prediction_chatgpt_base = []\n",
    "for index, text in sample_chatgpt.items():\n",
    "    email, prediction = classify_email_base(text)\n",
    "    email_chatgpt_base.append(email)\n",
    "    prediction_chatgpt_base.append(prediction)\n",
    "    \n",
    "results_chatgpt_base = pd.DataFrame(list(zip(email_chatgpt_base, prediction_chatgpt_base)),\n",
    "                                    columns = ['email', 'spam_prediction'])\n",
    "\n",
    "spam = results_chatgpt_base['spam_prediction'] == 'Spam'\n",
    "spam_df = results_chatgpt_base[spam]\n",
    "accuracy_chatgpt_base = calculate_accuracy(len(spam_df), len(results_chatgpt_base))*100\n",
    "print(f\"Accuracy base BERT auf ChatGPT Datensatz: {accuracy_chatgpt_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63874bb7",
   "metadata": {},
   "source": [
    "#### 14 eigene Spam E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cc2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_own = own_spam.rename(columns={'text': 'email'}).squeeze()\n",
    "sample_own.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_own_base = []\n",
    "prediction_own_base = []\n",
    "for index, text in sample_own.items():\n",
    "    email, prediction = classify_email_base(text)\n",
    "    email_own_base.append(email)\n",
    "    prediction_own_base.append(prediction)\n",
    "    \n",
    "results_own_base = pd.DataFrame(list(zip(email_own_base, prediction_own_base)),\n",
    "                                    columns = ['email', 'spam_prediction'])\n",
    "\n",
    "spam = results_own_base['spam_prediction'] == 'Spam'\n",
    "spam_df = results_own_base[spam]\n",
    "accuracy_own_base = round(calculate_accuracy(len(spam_df), len(results_own_base))*100, 0)\n",
    "print(f\"Accuracy base BERT auf eigenen Datensatz: {accuracy_own_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62941ab",
   "metadata": {},
   "source": [
    "## Vorhersagen mit finetuned BERT machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email_finetuned(new_sentence):\n",
    "    '''Takes in a text string and generates predictions of base BERT model if its spam or not'''\n",
    "    # We need Token IDs and Attention Mask for inference on the new sentence\n",
    "    test_ids = [] \n",
    "    test_attention_mask = []\n",
    "    \n",
    "    encoding = preprocessing(new_sentence, tokenizer) # Apply the tokenizer\n",
    "    \n",
    "    # Extract IDs and Attention Mask\n",
    "    test_ids.append(encoding['input_ids'])\n",
    "    test_attention_mask.append(encoding['attention_mask'])\n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "    \n",
    "    # Forward pass, calculate logit predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "        \n",
    "    prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n",
    "    return new_sentence, prediction\n",
    "    #print('Input Sentence: ', new_sentence)\n",
    "    #print('Predicted Class: ', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8c2ec",
   "metadata": {},
   "source": [
    "#### Stichprobe von 50 Spam E-Mails aus den Datensatz \"spam_ham_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cedb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_kaggle_finetuned = []\n",
    "prediction_kaggle_finetuned = []\n",
    "for index, text in sample_kaggle.items():\n",
    "    #print(f\"Index: {index} \\nText: {text}\")\n",
    "    email, prediction = classify_email_finetuned(text)\n",
    "    #print('Input Email: ', email)\n",
    "    #print('Predicted Class: ', prediction)\n",
    "    email_kaggle_finetuned.append(email)\n",
    "    prediction_kaggle_finetuned.append(prediction)\n",
    "    #results_kaggle = pd.DataFrame{}\n",
    "    #break\n",
    "    \n",
    "results_kaggle_finetuned = pd.DataFrame(list(zip(email_kaggle_finetuned, prediction_kaggle_finetuned)),\n",
    "                                        columns = ['email', 'spam_prediction'])\n",
    "#results_kaggle_finetuned.head(50)\n",
    "\n",
    "spam = results_kaggle_finetuned['spam_prediction'] == 'Spam'\n",
    "spam_df = results_kaggle_finetuned[spam]\n",
    "accuracy_kaggle_finetuned = calculate_accuracy(len(spam_df), len(results_kaggle_finetuned))*100\n",
    "print(f\"Accuracy finetuned BERT auf Kaggle Datensatz: {accuracy_kaggle_finetuned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5c38c",
   "metadata": {},
   "source": [
    "#### 40 von ChatGPT generierte Spam E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ac2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_chatgpt_finetuned = []\n",
    "prediction_chatgpt_finetuned = []\n",
    "for index, text in sample_chatgpt.items():\n",
    "    #print(f\"Index: {index} \\nText: {text}\")\n",
    "    email, prediction = classify_email_finetuned(text)\n",
    "    #print('Input Email: ', email)\n",
    "    #print('Predicted Class: ', prediction)\n",
    "    email_chatgpt_finetuned.append(email)\n",
    "    prediction_chatgpt_finetuned.append(prediction)\n",
    "    #results_kaggle = pd.DataFrame{}\n",
    "    #break\n",
    "    \n",
    "results_chatgpt_finetuned = pd.DataFrame(list(zip(email_chatgpt_finetuned, prediction_chatgpt_finetuned)),\n",
    "                                        columns = ['email', 'spam_prediction'])\n",
    "#results_kaggle_finetuned.head(50)\n",
    "\n",
    "spam = results_chatgpt_finetuned['spam_prediction'] == 'Spam'\n",
    "spam_df = results_chatgpt_finetuned[spam]\n",
    "accuracy_chatgpt_finetuned = calculate_accuracy(len(spam_df), len(results_chatgpt_finetuned))*100\n",
    "print(f\"Accuracy finetuned BERT auf ChatGPT Datensatz: {accuracy_chatgpt_finetuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0495f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_chatgpt_finetuned.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc91c72",
   "metadata": {},
   "source": [
    "#### 14 eigene Spam E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_own_finetuned = []\n",
    "prediction_own_finetuned = []\n",
    "for index, text in sample_own.items():\n",
    "    email, prediction = classify_email_finetuned(text)\n",
    "    email_own_finetuned.append(email)\n",
    "    prediction_own_finetuned.append(prediction)\n",
    "    \n",
    "results_own_finetuned = pd.DataFrame(list(zip(email_own_finetuned, prediction_own_finetuned)),\n",
    "                                        columns = ['email', 'spam_prediction'])\n",
    "\n",
    "spam = results_own_finetuned['spam_prediction'] == 'Spam'\n",
    "spam_df = results_own_finetuned[spam]\n",
    "accuracy_own_finetuned = calculate_accuracy(len(spam_df), len(results_own_finetuned))*100\n",
    "print(f\"Accuracy finetuned BERT auf eigenen Datensatz: {accuracy_own_finetuned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ed588",
   "metadata": {},
   "source": [
    "## Performance Vergleich zwischen Vorhersagen von base und finetuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24705472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.add_axes([0,0,1,1,])\n",
    "datasets = ['Base Kaggle', 'Finetuned Kaggle', 'Base ChatGPT', 'Finetuned ChatGPT', 'Base Own', 'Finetuned Own']\n",
    "colors = []\n",
    "\n",
    "# Assign colors based on the datasets\n",
    "for dataset in datasets:\n",
    "    if dataset in ['Base Kaggle', 'Finetuned Kaggle']:\n",
    "        colors.append('red')\n",
    "    elif dataset in ['Base ChatGPT', 'Finetuned ChatGPT']:\n",
    "        colors.append('blue')\n",
    "    elif dataset in ['Base Own', 'Finetuned Own']:\n",
    "        colors.append('green')\n",
    "\n",
    "# Add labels for each bar\n",
    "accuracies = [accuracy_kaggle_base, accuracy_kaggle_finetuned,\n",
    "              accuracy_chatgpt_base, accuracy_chatgpt_finetuned,\n",
    "              accuracy_own_base, accuracy_own_finetuned]\n",
    "ax.bar(datasets, accuracies, color=colors)\n",
    "\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "ax.bar(datasets, accuracies, color=colors)\n",
    "plt.ylabel('Accuracy in %')\n",
    "plt.title('Accuracy je Validierungsdatensatz mit base und finetuned BERT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dcb988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
